data  = {
    "SAP Developer":
    ["Yes, you can run a business warehouse without R/3 implementation. You have to simply transfer structures associated with business warehouse data sources (ODS table, Infocube) to the inbound data files or use third-party tools to connect your flat files and other data sources.Different layers in the R/3 system includes Presentation Layer Database layer Application layer",
    "To create a table in the data dictionary, you have to follow this step.Creating domains (data type, field length, range) . Creating data elements (properties and type for a table field) . Creating tables (SE 11)",
   "The SAP smart forms tool is an excellent way to send and print important documents. I use this tool to create forms, emails, PDF files and documents that I plan to share with clients and coworkers online because it makes it easy to create the layout and maintain the logic of forms. It also allows me to make quick updates by using graphical tools instead of having to program changes manually. If I'm collaborating on a project with someone who isn't familiar with programming, this is especially important because the SAP smart tools allow them to enter data easily.There are also several form templates the SAP smart tools provide to streamline processes for sales and distribution (SD), customer relationship management (CRM), human resources (HR) and financial accounting."],

   "DotNet Developer":[
       "Yes, Both Encapsulation and Abstraction do the same thing but with few differences Encapsulation mainly encapsulates the object and so hides the details as well as it binds the data.So Encapsulation = Hiding + Binding the data ",
       "When the Garbage Collector gets called by the CLR to DE-allocate the memory in the heap, the Garbage Collector start finding the references of all the reachable objects which are currently in use. So it find the objects which are used by the processes and for rest of objects which are un-reachable or the Garbage collector is not able to find the references for them, it marks them for deletion.    Here the Garbage collector makes an Object graph which keeps track of all the objects which are marked for deletion. After the deleting the references for those objects, the heap memory gets compacted and a new root becomes available to use by the new created object. No, this object graph creates virtually by the Garbage Collector to keep all the objects and to make them for deletion. This is the Garbage Collector responsibility to create this object graph and gets the references of each reachable object which are used by the application",
       "The main difference between IIS and Kestrel is that Kestrel is a cross-platform server. It runs on Windows, Linux, and Mac, whereas IIS only runs on Windows.Another essential difference between the two is that Kestrel is fully open-source, whereas IIS is closed-source and developed and maintained only by Microsoft.IIS is very old software and comes with a considerable legacy and bloat. With Kestrel, Microsoft started with high-performance in mind. They developed it from scratch, which allowed them to ignore the legacy/compatibility issues and focus on speed and efficiency.However, Kestrel doesn’t provide all the rich functionality of a full-fledged web server such as IIS, Nginx, or Apache. Hence, we typically use it as an application server, with one of the above servers acting as a reverse proxy. "
   ],
   "Data Science":[
       'Time series data is known to posses linearity. On the other hand, a decision tree algorithm is known to work best to detect non – linear interactions. The reason why decision tree failed to provide robust predictions because it couldn’t map the linear relationship as good as a regression model did. Therefore, we learned that, a linear regression model can provide robust prediction given the data set satisfies its linearity assumptions.',
       'Low bias occurs when the model’s predicted values are near to actual values. In other words, the model becomes flexible enough to mimic the training data distribution. While it sounds like great achievement, but not to forget, a flexible model has no generalization capabilities. It means, when this model is tested on an unseen data, it gives disappointing results.In such situations, we can use bagging algorithm (like random forest) to tackle high variance problem. Bagging algorithms divides a data set into subsets made with repeated randomized sampling. Then, these samples are used to generate  a set of models using a single learning algorithm. Later, the model predictions are combined using voting (classification) or averaging (regression).Also, to combat high variance, we can Use regularization technique, where higher model coefficients get penalized, hence lowering model complexity. Use top n features from variable importance chart. May be, with all the variable in the data set, the algorithm is having difficulty in finding the meaningful signal.',
       'As we know, ensemble learners are based on the idea of combining weak learners to create strong learners. But, these learners provide superior results when the combined models are uncorrelated. Since, we have used 5 GBM models and got no accuracy improvement, suggests that the models are correlated. The problem with correlated models is, all the models provide same information.For example: If model 1 has classified User1122 as 1, there are high chances model 2 and model 3 would have done the same, even if its actual value is 0. Therefore, ensemble learners are built on the premise of combining weak uncorrelated models to obtain better predictions.'
   ],
   "DevOps Engineer":['There are many industries that are using DevOps so you can mention any of those use cases, you can also refer the below example:Etsy is a peer-to-peer e-commerce website focused on handmade or vintage items and supplies, as well as unique factory-manufactured items. Etsy struggled with slow, painful site updates that frequently caused the site to go down. It affected sales for millions of Etsy’s users who sold goods through online market place and risked driving them to the competitor.With the help of a new technical management team, Etsy transitioned from its waterfall model, which produced four-hour full-site deployments twice weekly, to a more agile approach. Today, it has a fully automated deployment pipeline, and its continuous delivery practices have reportedly resulted in more than 50 deployments a day with fewer disruptions.',
                    'DevOps engineers almost always work in a 24/7 business-critical online environment. I was adaptable to on-call duties and was available to take up real-time, live-system responsibility. I successfully automated processes to support continuous software deployments. I have experience with public/private clouds, tools like Chef or Puppet, scripting and automation with tools like Python and PHP, and a background in Agile.',
                    'There are three ways to configure a script to run every time a repository receives new commits through push, one needs to define either a pre-receive, update, or a post-receive hook depending on when exactly the script needs to be triggered.Pre-receive hook in the destination repository is invoked when commits are pushed to it. Any script bound to this hook will be executed before any references are updated. This is a useful hook to run scripts that help enforce development policies.Update hook works in a similar manner to pre-receive hook, and is also triggered before any updates are actually made. However, the update hook is called once for every commit that has been pushed to the destination repository.Finally, post-receive hook in the repository is invoked after the updates have been accepted into the destination repository. This is an ideal place to configure simple deployment scripts, invoke some continuous integration systems, dispatch notification emails to repository maintainers, etc.Hooks are local to every Git repository and are not versioned. Scripts can either be created within the hooks directory inside the “.git” directory, or they can be created elsewhere and links to those scripts can be placed within the directory.'
                    ]
}